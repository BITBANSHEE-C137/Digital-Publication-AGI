TL;DR

What if the people responsible for keeping AI safe are losing the ability to do so, not because AI is too powerful, but because we've already stopped thinking for ourselves?
This paper introduces the Safety Inversion: as AI systems grow more capable, the humans tasked with overseeing them are becoming measurably less equipped for the job. PIAAC and NAEP data show that the specific skills oversight requires (sustained analytical reading, proportional reasoning, independent source evaluation) peaked in the U.S. population around 2000 and have declined since.
The decline isn't about getting dumber. It's a cognitive recomposition: newer cohorts gained faster pattern recognition, interface fluency, and multi-system coordination, skills optimized for collaboration with AI. What eroded are the skills required for supervision of AI. Those are different relationships, and they require different cognitive toolkits.
The paper defines five behavioral pillars for AGI and identifies Pillar 4 (persistent memory and belief revision) as the critical fault line. Not because it can't be engineered, but because a system that genuinely remembers, updates its beliefs, and maintains coherent identity over time is a system that forms preferences, develops judgment, and resists correction. Industry is building memory as a feature. It is not building memory as cognition.
Three dynamics are converging: the capability gap is widening, oversight capacity is narrowing, and market incentives are fragmenting AI into monetizable tools rather than integrated intelligence. The result is a population optimized to use AI but not equipped to govern it, building systems too capable to oversee, operated by a population losing the capacity to try.
Written from 30 years inside the machine, from encrypted satellite communications in forward-deployed combat zones to enterprise cloud architecture, this is a thought experiment about what happens when we burn the teletypes.